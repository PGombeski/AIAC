# 🍕 LLM Image Reasoning Consistency Test

This repository contains a small experiment testing **consistency in large language model (LLM) image reasoning** by rating pizzas from three famous pizzerias.

## 📖 Overview
Most computer systems are **1:1 mapping**: the same input → same output.  
LLMs are **1:many mapping**: the same input → potentially different outputs.  

We wanted to answer:
> "If you score the same pizza photo multiple times, will the LLM rate it the same way?"

We tested this by running **30 evaluations per pizza** at two temperature settings:
- **0** → more deterministic
- **1** → more variable

---

## 🧪 Methodology
1. **Prompt Design**
   - Input: *"On a scale of 1 to 10, how delicious does this pizza look? Respond with numbers only."*
   - Variants tested: `"1 to 10"` vs `"1.0 to 10.0"`
2. **Data Collection**
   - 3 pizzerias tested
   - 30 runs each, at temperature=0 and temperature=1
3. **Evaluation**
   - Collected outputs in CSV
   - Calculated mean, standard deviation, and compared rankings

---

## 📊 Results

| Pizzeria | Temp=0 Mean | Temp=0 StdDev | Temp=1 Mean | Temp=1 StdDev |
|----------|-------------|---------------|-------------|---------------|
| A        | 8.2         | 0.00          | 8.24        | 0.30          |
| B        | 8.2         | 0.00          | 8.15        | 0.34          |
| C        | 7.5         | 0.00          | 7.47        | 0.18          |

### Key Takeaways
- **Temperature = 0** produced identical scores for each pizza.
- **Temperature = 1** allowed small variation in scores.
- `"1.0 to 10.0"` prompted finer-grained ratings than `"1 to 10"`.
- Consistency can hide subtle preferences; variability can surface them.
- LLM seemed to favor **thin crust over Chicago deep dish** (model bias? Or good taste?).
- Lower Temperature seemed to suppress some minor score differences, suggesting a trade-off between consistency and precision

---
